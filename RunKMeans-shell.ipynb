{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75829fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from operator import add\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.rdd import RDD\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e86392d8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aac24dc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "rawData = sc.textFile(\"kddcup.corrected.csv\").sample(False, 0.01)\n",
    "# 1 percent of data , randomly sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9f5a8be",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/aldof/kddcup.corrected.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m last_column_values \u001b[38;5;241m=\u001b[39m rawData\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# sort them in descending order\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m sorted_counts \u001b[38;5;241m=\u001b[39m \u001b[43mlast_column_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcountByValue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m      6\u001b[0m sorted_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(sorted_counts, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value, count \u001b[38;5;129;01min\u001b[39;00m sorted_counts:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:1457\u001b[0m, in \u001b[0;36mRDD.countByValue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m         m1[k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m1\n\u001b[0;32m-> 1457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountPartition\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmergeMaps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:999\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m--> 999\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;124;03mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;124;03mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 950\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/aldof/kddcup.corrected.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# split line by comma and get the last element\n",
    "last_column_values = rawData.map(lambda x: x.split(',')[-1])\n",
    "\n",
    "# sort them in descending order\n",
    "sorted_counts = last_column_values.countByValue().items()\n",
    "sorted_counts = sorted(sorted_counts, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for value, count in sorted_counts:\n",
    "    print(f\"{value}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebb111c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract labels and vectors\n",
    "labelsAndData = rawData.map(lambda line: line.split(',')).map(lambda arr: (arr[-1], Vectors.dense(arr[0:1] + arr[4:-1])))\n",
    "data = labelsAndData.values().cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3681437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('normal.', DenseVector([0.0, 224.0, 296.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 7.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 34.0, 125.0, 1.0, 0.0, 0.03, 0.04, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 229.0, 1911.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 13.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 40.0, 131.0, 1.0, 0.0, 0.03, 0.04, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 251.0, 294.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 15.0, 15.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 15.0, 251.0, 1.0, 0.0, 0.07, 0.03, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 336.0, 520.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 60.0, 255.0, 1.0, 0.0, 0.02, 0.05, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 266.0, 19790.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 7.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 17.0, 255.0, 1.0, 0.0, 0.06, 0.05, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 235.0, 3609.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 8.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.25, 179.0, 255.0, 1.0, 0.0, 0.01, 0.02, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 238.0, 468.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.0, 18.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.11, 189.0, 255.0, 1.0, 0.0, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 314.0, 2298.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 9.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.22, 251.0, 255.0, 1.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 167.0, 164.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 4.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 3.0, 255.0, 1.0, 0.0, 0.33, 0.01, 0.0, 0.0, 0.0, 0.0]))\n",
      "('normal.', DenseVector([0.0, 164.0, 3826.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 255.0, 1.0, 0.0, 1.0, 0.04, 0.0, 0.0, 0.0, 0.0]))\n"
     ]
    }
   ],
   "source": [
    "sample_data = labelsAndData.take(10)  # Take the first 10 elements as a sample\n",
    "for data_point in sample_data:\n",
    "    print(data_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddaacdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,224.0,296.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.0,7.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,34.0,125.0,1.0,0.0,0.03,0.04,0.0,0.0,0.0,0.0]\n",
      "[0.0,229.0,1911.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,13.0,13.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,40.0,131.0,1.0,0.0,0.03,0.04,0.0,0.0,0.0,0.0]\n",
      "[0.0,251.0,294.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,15.0,15.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,15.0,251.0,1.0,0.0,0.07,0.03,0.0,0.0,0.0,0.0]\n",
      "[0.0,336.0,520.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,2.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,60.0,255.0,1.0,0.0,0.02,0.05,0.0,0.0,0.0,0.0]\n",
      "[0.0,266.0,19790.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,7.0,7.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,17.0,255.0,1.0,0.0,0.06,0.05,0.0,0.0,0.0,0.0]\n",
      "[0.0,235.0,3609.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,8.0,0.0,0.0,0.0,0.0,1.0,0.0,0.25,179.0,255.0,1.0,0.0,0.01,0.02,0.0,0.0,0.0,0.0]\n",
      "[0.0,238.0,468.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,12.0,18.0,0.0,0.0,0.0,0.0,1.0,0.0,0.11,189.0,255.0,1.0,0.0,0.01,0.01,0.0,0.0,0.0,0.0]\n",
      "[0.0,314.0,2298.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,3.0,9.0,0.0,0.0,0.0,0.0,1.0,0.0,0.22,251.0,255.0,1.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0]\n",
      "[0.0,167.0,164.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,4.0,0.0,0.0,0.0,0.0,1.0,0.0,0.5,3.0,255.0,1.0,0.0,0.33,0.01,0.0,0.0,0.0,0.0]\n",
      "[0.0,164.0,3826.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,255.0,1.0,0.0,1.0,0.04,0.0,0.0,0.0,0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sample_data = data.take(10)  # Take the first 10 elements as a sample\n",
    "for data_point in sample_data:\n",
    "    print(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1684300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.mllib. is for rdd and pyspark.ml. is for dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fae3edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.70676566e+01 7.83402095e+02 5.73928275e+02 0.00000000e+00\n",
      " 2.84784378e-04 0.00000000e+00 1.75345810e-02 6.10252238e-05\n",
      " 1.46257120e-01 1.09845403e-03 0.00000000e+00 0.00000000e+00\n",
      " 5.71602929e-03 6.91619203e-04 6.10252238e-05 8.13669650e-04\n",
      " 0.00000000e+00 0.00000000e+00 1.01708706e-03 3.34202563e+02\n",
      " 2.94571562e+02 1.78253051e-01 1.78375915e-01 5.70616355e-02\n",
      " 5.71122864e-02 7.89549227e-01 2.11096420e-02 2.72377950e-02\n",
      " 2.32806530e+02 1.89214382e+02 7.53523596e-01 3.01722945e-02\n",
      " 6.03213385e-01 6.14727421e-03 1.78470708e-01 1.78260374e-01\n",
      " 5.71606998e-02 5.69426363e-02]\n",
      "[5.80000000e+01 8.88000000e+02 1.90086867e+06 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 9.10000000e+01 1.70000000e+01 2.26666667e-01 5.33333333e-02\n",
      " 1.33333333e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans()\n",
    "model = kmeans.train(data, k = 2)         # k = number of clusters\n",
    "cluster_centers = model.clusterCenters    # cluster centers\n",
    "\n",
    "for c in cluster_centers:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8dd0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===============================>                       (13 + 10) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0back.                   28\n",
      "0ipsweep.               103\n",
      "0neptune.             10791\n",
      "0nmap.                   23\n",
      "0normal.               9851\n",
      "0pod.                     5\n",
      "0portsweep.              83\n",
      "0satan.                 144\n",
      "0smurf.               28115\n",
      "0spy.                     1\n",
      "0teardrop.                3\n",
      "0warezclient.            13\n",
      "1normal.                  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# maps each (label, vector) tuple to (cluster, label)\n",
    "cluster_label_rdd = labelsAndData.map(lambda x: (model.predict(x[1]), x[0]))\n",
    "\n",
    "# Count (cluster, label) pair\n",
    "clusterLabelCount = cluster_label_rdd.countByValue()\n",
    "\n",
    "for ((cluster, label), count) in sorted(clusterLabelCount.items()):\n",
    "    print(f\"{cluster}{label:18}{count:8}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046ee80",
   "metadata": {},
   "source": [
    "#### ------------ Use this to Dump the Non-Normalized Data & Clusters -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90d0803d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans()\n",
    "model = kmeans.train(data, k = 100)\n",
    "\n",
    "# maps each vector to (cluster, vector) and concatenate cluster and vector\n",
    "sample = data.map(lambda vector: (model.predict(vector), vector.toArray())).map(lambda x: (x[0], \",\".join(map(str, x[1]))))\n",
    "sample.sample(False, 0.01).saveAsTextFile(\"./kmeans-sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8d3b9",
   "metadata": {},
   "source": [
    "###  ------------ Investigate the Average Distance to Closest Centroid  ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a5945f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 519:========================================>              (17 + 6) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 269.62675538366244)\n",
      "(40, 133.9497731323642)\n",
      "(50, 186.1093867782195)\n",
      "(60, 232.90671692144602)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculates the distance between vectors\n",
    "def distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "# calculates the distance to centroid\n",
    "def distToCentroid(vector, model):\n",
    "    cluster = model.predict(vector)\n",
    "    centroid = model.clusterCenters[cluster]\n",
    "    return distance(centroid, vector)\n",
    "\n",
    "# calculates clustering score\n",
    "def clusteringScore(data, k):\n",
    "    kmeans = KMeans.train(data, k)\n",
    "    return data.map(lambda vector: distToCentroid(vector, kmeans)).mean()\n",
    "\n",
    "results = []\n",
    "for k in range(30, 61, 10):\n",
    "    score = clusteringScore(data, k)\n",
    "    results.append((k, score))\n",
    "\n",
    "for r in results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4316d35",
   "metadata": {},
   "source": [
    "### ------------ Normalize the Data and Compare Clustering Scores --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7553b17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1018:=======================================>              (17 + 6) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 0.36583083690398577)\n",
      "(70, 0.3306916741201658)\n",
      "(80, 0.30956256007676186)\n",
      "(90, 0.2907341227802317)\n",
      "(100, 0.2770668724341242)\n",
      "(110, 0.24904216657098613)\n",
      "(120, 0.23244947833475677)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[625] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    data_as_array = data.map(lambda x: x.toArray())\n",
    "    num_cols = len(data_as_array.first())\n",
    "    n = data_as_array.count()\n",
    "    sums = data_as_array.reduce(lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "    sum_squares = data_as_array.aggregate([0.0] * num_cols,\n",
    "                                          lambda a, b: [x + y * y for x, y in zip(a, b)],\n",
    "                                          lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "    stdevs = [math.sqrt(n * sum_sq - sum_ * sum_) / n for sum_sq, sum_ in zip(sum_squares, sums)]\n",
    "    means = [sum_ / n for sum_ in sums]\n",
    "\n",
    "    def normalize_vector(vector):\n",
    "        normalized_array = [(value - mean) / stdev if stdev > 0 else value - mean\n",
    "                            for value, mean, stdev in zip(vector.toArray(), means, stdevs)]\n",
    "        return Vectors.dense(normalized_array)\n",
    "\n",
    "    return normalize_vector\n",
    "\n",
    "normalized_data = data.map(normalize(data)).cache()\n",
    "\n",
    "\n",
    "clustering_scores = [(k, clusteringScore(normalized_data, k)) for k in range(60, 121, 10)]\n",
    "\n",
    "\n",
    "for score in clustering_scores:\n",
    "    print(score)\n",
    "\n",
    "normalized_data.unpersist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e51dd9",
   "metadata": {},
   "source": [
    "### ------------ Switch to One-Hot Encoding of Categorical Attributes  ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbdfaa2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def onehot(raw_data):\n",
    "    splitData = raw_data.map(lambda x: x.split(','))\n",
    "    protocols = splitData.map(lambda x: x[1]).distinct().zipWithIndex().collectAsMap()\n",
    "    print(\"PROTOCOLS\", protocols)\n",
    "    services = splitData.map(lambda x: x[2]).distinct().zipWithIndex().collectAsMap()\n",
    "    print(\"SERVICES\", services)\n",
    "    tcpStates = splitData.map(lambda x: x[3]).distinct().zipWithIndex().collectAsMap()\n",
    "    print(\"TCPSTATES\", tcpStates)\n",
    "\n",
    "    def encode_line(line):\n",
    "        buffer = line.split(',')\n",
    "        protocol = buffer.pop(1)\n",
    "        service = buffer.pop(1)\n",
    "        tcpState = buffer.pop(1)\n",
    "        label = buffer.pop(-1)\n",
    "        vector = [float(x) for x in buffer]\n",
    "\n",
    "        newProtocolFeatures = [0.0] * len(protocols)\n",
    "        newProtocolFeatures[protocols[protocol]] = 1.0\n",
    "        newServiceFeatures = [0.0] * len(services)\n",
    "        newServiceFeatures[services[service]] = 1.0\n",
    "        newTcpStateFeatures = [0.0] * len(tcpStates)\n",
    "        newTcpStateFeatures[tcpStates[tcpState]] = 1.0\n",
    "\n",
    "        vector[1:1] = newTcpStateFeatures\n",
    "        vector[1:1] = newServiceFeatures\n",
    "        vector[1:1] = newProtocolFeatures\n",
    "\n",
    "        return label, Vectors.dense(vector)\n",
    "\n",
    "    return encode_line\n",
    "\n",
    "print(\"2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48530bf",
   "metadata": {},
   "source": [
    "### ------------- Dump the Normalized and One-Hot Encoded Data  ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef7dae16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROTOCOLS {'tcp': 0, 'udp': 1, 'icmp': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICES {'time': 0, 'courier': 1, 'pop_3': 2, 'smtp': 3, 'ftp': 4, 'http_443': 5, 'login': 6, 'bgp': 7, 'IRC': 8, 'link': 9, 'shell': 10, 'csnet_ns': 11, 'gopher': 12, 'Z39_50': 13, 'eco_i': 14, 'efs': 15, 'rje': 16, 'name': 17, 'systat': 18, 'urh_i': 19, 'uucp': 20, 'domain': 21, 'sunrpc': 22, 'whois': 23, 'remote_job': 24, 'discard': 25, 'finger': 26, 'ntp_u': 27, 'other': 28, 'ldap': 29, 'kshell': 30, 'echo': 31, 'ctf': 32, 'sql_net': 33, 'iso_tsap': 34, 'nntp': 35, 'ssh': 36, 'hostnames': 37, 'telnet': 38, 'klogin': 39, 'mtp': 40, 'netbios_dgm': 41, 'urp_i': 42, 'supdup': 43, 'nnsp': 44, 'netbios_ssn': 45, 'imap4': 46, 'netstat': 47, 'domain_u': 48, 'auth': 49, 'private': 50, 'netbios_ns': 51, 'exec': 52, 'uucp_path': 53, 'http': 54, 'ecr_i': 55, 'ftp_data': 56, 'vmnet': 57, 'pop_2': 58, 'printer': 59, 'daytime': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCPSTATES {'OTH': 0, 'S3': 1, 'SF': 2, 'REJ': 3, 'RSTO': 4, 'SH': 5, 'S0': 6, 'S2': 7, 'S1': 8, 'RSTR': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1227] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parseFunction = onehot(rawData)\n",
    "labelsAndData = rawData.map(parseFunction)\n",
    "normalizedLabelsAndData = labelsAndData.mapValues(normalize(labelsAndData.values())).cache()\n",
    "\n",
    "kmeans = KMeans()\n",
    "model = kmeans.train(normalizedLabelsAndData.values(), k = 100)\n",
    "sample = normalizedLabelsAndData.values().map(lambda vector: str(model.predict(vector)) + \",\" + \",\".join(map(str, vector.toArray()))).sample(False, 0.01)\n",
    "sample.saveAsTextFile(\"./kmeans-sample-normalized\")\n",
    "normalizedLabelsAndData.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0ba53",
   "metadata": {},
   "source": [
    "### ------------ Switch to Entropy-Based Clustering Scores  --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f1034a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROTOCOLS {'tcp': 0, 'udp': 1, 'icmp': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICES {'time': 0, 'courier': 1, 'pop_3': 2, 'smtp': 3, 'ftp': 4, 'http_443': 5, 'login': 6, 'bgp': 7, 'IRC': 8, 'link': 9, 'shell': 10, 'csnet_ns': 11, 'gopher': 12, 'Z39_50': 13, 'eco_i': 14, 'efs': 15, 'rje': 16, 'name': 17, 'systat': 18, 'urh_i': 19, 'uucp': 20, 'domain': 21, 'sunrpc': 22, 'whois': 23, 'remote_job': 24, 'discard': 25, 'finger': 26, 'ntp_u': 27, 'other': 28, 'ldap': 29, 'kshell': 30, 'echo': 31, 'ctf': 32, 'sql_net': 33, 'iso_tsap': 34, 'nntp': 35, 'ssh': 36, 'hostnames': 37, 'telnet': 38, 'klogin': 39, 'mtp': 40, 'netbios_dgm': 41, 'urp_i': 42, 'supdup': 43, 'nnsp': 44, 'netbios_ssn': 45, 'imap4': 46, 'netstat': 47, 'domain_u': 48, 'auth': 49, 'private': 50, 'netbios_ns': 51, 'exec': 52, 'uucp_path': 53, 'http': 54, 'ecr_i': 55, 'ftp_data': 56, 'vmnet': 57, 'pop_2': 58, 'printer': 59, 'daytime': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCPSTATES {'OTH': 0, 'S3': 1, 'SF': 2, 'REJ': 3, 'RSTO': 4, 'SH': 5, 'S0': 6, 'S2': 7, 'S1': 8, 'RSTR': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1446:=======================================>              (17 + 6) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 68.66310225148231)\n",
      "(130, 67.51713942968327)\n",
      "(140, 68.51568334084513)\n",
      "(150, 67.4680657891172)\n",
      "(160, 66.65860496116079)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "def entropy(counts):\n",
    "    values = [c for c in counts if c > 0]\n",
    "    n = sum(values)\n",
    "    return sum(-p * math.log(p) for v in values for p in [v / n])\n",
    "\n",
    "def clusteringScoreByEntropy(normalizedData, k):\n",
    "    normalizedData.cache()\n",
    "\n",
    "    kmeans = KMeans()\n",
    "    model = kmeans.train(normalizedData.values(), k)\n",
    "\n",
    "    normalizedData.unpersist()\n",
    "\n",
    "    labelsAndClusters = normalizedData.mapValues(lambda x: model.predict(x))\n",
    "    clustersAndLabels = labelsAndClusters.map(lambda x: (x[1], x[0]))\n",
    "    labelsInCluster = clustersAndLabels.groupByKey().values()\n",
    "    labelCounts = labelsInCluster.map(lambda x: list(map(len, x))).map(lambda x: [sum(x)] + x)\n",
    "    n = normalizedData.count()\n",
    "\n",
    "    return labelCounts.map(lambda m: sum(m) * entropy(m)).sum() / n\n",
    "\n",
    "\n",
    "parseFunction = onehot(rawData)\n",
    "labelsAndData = rawData.map(parseFunction)\n",
    "normalizeFunction = normalize(labelsAndData.values())\n",
    "normalizedLabelsAndData = labelsAndData.mapValues(normalizeFunction).cache()\n",
    "\n",
    "results = [(k, clusteringScoreByEntropy(normalizedLabelsAndData, k)) for k in range(120, 161, 10)]\n",
    "for r in results:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecaf93f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1501:=======================================>              (17 + 6) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0nmap.                    2\n",
      "0normal.                728\n",
      "0satan.                  11\n",
      "1normal.               2315\n",
      "2neptune.              8230\n",
      "3ipsweep.                24\n",
      "4neptune.                 5\n",
      "5normal.                  1\n",
      "6portsweep.              17\n",
      "6satan.                 106\n",
      "7neptune.                 2\n",
      "8normal.                  2\n",
      "9normal.                 31\n",
      "9smurf.                1286\n",
      "10neptune.                16\n",
      "11neptune.                11\n",
      "12neptune.                 8\n",
      "13normal.               1801\n",
      "14neptune.                 6\n",
      "15neptune.                 7\n",
      "16neptune.                11\n",
      "17neptune.                15\n",
      "18normal.                  1\n",
      "19ipsweep.                 2\n",
      "19neptune.              1021\n",
      "19portsweep.               3\n",
      "20neptune.                11\n",
      "21neptune.                12\n",
      "22normal.                  1\n",
      "22portsweep.               2\n",
      "23neptune.                14\n",
      "23normal.                  1\n",
      "24neptune.               928\n",
      "24portsweep.               2\n",
      "25normal.                227\n",
      "26neptune.                11\n",
      "27normal.                389\n",
      "28neptune.                 4\n",
      "29normal.                  2\n",
      "30neptune.                 4\n",
      "31neptune.                17\n",
      "32neptune.                12\n",
      "33neptune.                11\n",
      "34normal.                489\n",
      "35neptune.                 9\n",
      "35normal.                 15\n",
      "36normal.                 38\n",
      "37neptune.                12\n",
      "38neptune.                 4\n",
      "39neptune.                10\n",
      "40neptune.                 1\n",
      "41nmap.                    1\n",
      "42neptune.                 8\n",
      "43normal.                  3\n",
      "44portsweep.               5\n",
      "45normal.                 18\n",
      "46normal.                 25\n",
      "47neptune.                 8\n",
      "48normal.                  1\n",
      "49neptune.                 7\n",
      "49portsweep.               1\n",
      "50normal.                  1\n",
      "51neptune.                10\n",
      "52normal.                 40\n",
      "53normal.                  6\n",
      "54satan.                   1\n",
      "55normal.                 31\n",
      "56neptune.                 4\n",
      "57normal.                192\n",
      "58neptune.                 9\n",
      "58normal.                  4\n",
      "59neptune.                13\n",
      "60neptune.                 2\n",
      "61neptune.                14\n",
      "62neptune.                14\n",
      "63neptune.                14\n",
      "64normal.                  1\n",
      "65normal.                  3\n",
      "66nmap.                    1\n",
      "67neptune.                 5\n",
      "68normal.                 54\n",
      "69neptune.                 7\n",
      "70neptune.                10\n",
      "71neptune.                17\n",
      "71normal.                350\n",
      "71warezclient.             7\n",
      "72nmap.                   12\n",
      "73teardrop.                3\n",
      "74neptune.                 7\n",
      "75neptune.                 3\n",
      "76neptune.                 3\n",
      "76normal.                  2\n",
      "77pod.                     5\n",
      "78neptune.                11\n",
      "79neptune.                 6\n",
      "80neptune.                12\n",
      "81portsweep.              28\n",
      "82neptune.                 1\n",
      "82normal.                 54\n",
      "83normal.                371\n",
      "84normal.                  1\n",
      "85neptune.                15\n",
      "86normal.                 19\n",
      "86warezclient.             6\n",
      "87smurf.               26829\n",
      "88neptune.                 7\n",
      "88normal.                  6\n",
      "89neptune.                13\n",
      "89portsweep.               1\n",
      "90normal.                 33\n",
      "91neptune.                19\n",
      "91normal.                  1\n",
      "92normal.                108\n",
      "93neptune.                 8\n",
      "94neptune.                 5\n",
      "94normal.                 11\n",
      "94satan.                   4\n",
      "95ipsweep.                70\n",
      "95nmap.                    6\n",
      "96neptune.                 9\n",
      "97neptune.                11\n",
      "98neptune.                 8\n",
      "98portsweep.               1\n",
      "99neptune.                22\n",
      "99normal.                 18\n",
      "99spy.                     1\n",
      "100neptune.                13\n",
      "101neptune.                 8\n",
      "102normal.                782\n",
      "103neptune.                10\n",
      "104normal.                248\n",
      "105ipsweep.                 6\n",
      "105normal.                  1\n",
      "106back.                   28\n",
      "106normal.                  1\n",
      "107neptune.                19\n",
      "108neptune.                 3\n",
      "108normal.                  7\n",
      "109normal.                  1\n",
      "110normal.                 20\n",
      "111portsweep.               1\n",
      "112normal.                 40\n",
      "113normal.                  2\n",
      "114normal.                114\n",
      "115normal.                 63\n",
      "116normal.                 25\n",
      "117normal.                 78\n",
      "118normal.                222\n",
      "119neptune.                 1\n",
      "119portsweep.               1\n",
      "120ipsweep.                 1\n",
      "120nmap.                    1\n",
      "120normal.                  1\n",
      "121portsweep.               5\n",
      "122neptune.                20\n",
      "122normal.                  7\n",
      "122portsweep.               1\n",
      "122satan.                   4\n",
      "123normal.                  2\n",
      "124neptune.                 4\n",
      "125portsweep.              11\n",
      "126normal.                514\n",
      "127normal.                  3\n",
      "128normal.                 83\n",
      "129normal.                  1\n",
      "130portsweep.               4\n",
      "131normal.                  2\n",
      "132normal.                 22\n",
      "133normal.                 14\n",
      "134normal.                  2\n",
      "135normal.                  2\n",
      "136normal.                 62\n",
      "137normal.                  1\n",
      "138neptune.                 8\n",
      "139normal.                 64\n",
      "140neptune.                 1\n",
      "141normal.                  1\n",
      "142normal.                  4\n",
      "143neptune.                 8\n",
      "144normal.                 17\n",
      "145satan.                  18\n",
      "146neptune.                 1\n",
      "147normal.                 17\n",
      "148neptune.                 1\n",
      "149normal.                 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[1383] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = KMeans.train(normalizedLabelsAndData.values(), 150)\n",
    "\n",
    "\n",
    "clusterLabelCount = labelsAndData.map(lambda x: (model.predict(normalizeFunction(x[1])), x[0])) \\\n",
    "    .countByValue()\n",
    "\n",
    "\n",
    "for ((cluster, label), count) in sorted(clusterLabelCount.items()):\n",
    "    print(f\"{cluster:<1}{label:18}{count:8}\")\n",
    "\n",
    "normalizedLabelsAndData.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a734e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROTOCOLS {'tcp': 0, 'udp': 1, 'icmp': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERVICES {'time': 0, 'courier': 1, 'pop_3': 2, 'smtp': 3, 'ftp': 4, 'http_443': 5, 'login': 6, 'bgp': 7, 'IRC': 8, 'link': 9, 'shell': 10, 'csnet_ns': 11, 'gopher': 12, 'Z39_50': 13, 'eco_i': 14, 'efs': 15, 'rje': 16, 'name': 17, 'systat': 18, 'urh_i': 19, 'uucp': 20, 'domain': 21, 'sunrpc': 22, 'whois': 23, 'remote_job': 24, 'discard': 25, 'finger': 26, 'ntp_u': 27, 'other': 28, 'ldap': 29, 'kshell': 30, 'echo': 31, 'ctf': 32, 'sql_net': 33, 'iso_tsap': 34, 'nntp': 35, 'ssh': 36, 'hostnames': 37, 'telnet': 38, 'klogin': 39, 'mtp': 40, 'netbios_dgm': 41, 'urp_i': 42, 'supdup': 43, 'nnsp': 44, 'netbios_ssn': 45, 'imap4': 46, 'netstat': 47, 'domain_u': 48, 'auth': 49, 'private': 50, 'netbios_ns': 51, 'exec': 52, 'uucp_path': 53, 'http': 54, 'ecr_i': 55, 'ftp_data': 56, 'vmnet': 57, 'pop_2': 58, 'printer': 59, 'daytime': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCPSTATES {'OTH': 0, 'S3': 1, 'SF': 2, 'REJ': 3, 'RSTO': 4, 'SH': 5, 'S0': 6, 'S2': 7, 'S1': 8, 'RSTR': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0,tcp,auth,SF,9,37,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,1,0.00,0.00,0.00,0.00,0.50,1.00,0.00,69,22,0.07,0.06,0.01,0.18,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,other,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,1.00,1.00,1.00,0.00,0.00,2,2,1.00,0.00,0.50,0.00,0.00,0.00,1.00,1.00,normal.',\n",
       " '0,tcp,other,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,1.00,1.00,1.00,0.00,0.00,3,3,1.00,0.00,0.33,0.00,0.00,0.00,1.00,1.00,normal.',\n",
       " '27,tcp,ftp,SF,926,2720,0,0,0,19,0,1,0,0,0,0,0,0,0,0,0,1,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,143,44,0.31,0.02,0.01,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,auth,SF,9,37,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,1,0.00,0.00,0.00,0.00,0.50,1.00,0.00,234,16,0.06,0.02,0.00,0.12,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,auth,SF,10,39,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2,1,0.00,0.00,0.00,0.00,0.50,1.00,0.00,104,5,0.05,0.05,0.01,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,other,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,1.00,1.00,1.00,0.00,0.00,8,8,1.00,0.00,0.12,0.00,0.00,0.00,1.00,1.00,normal.',\n",
       " '0,tcp,other,REJ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0.00,0.00,1.00,1.00,1.00,0.00,0.00,20,20,1.00,0.00,0.05,0.00,0.00,0.00,1.00,1.00,normal.',\n",
       " '27,tcp,ftp,SF,931,2720,0,0,0,19,0,1,0,0,0,0,0,0,0,0,0,1,1,1,0.00,0.00,0.00,0.00,1.00,0.00,0.00,122,44,0.36,0.04,0.01,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " '0,tcp,finger,SF,7,222,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,2,0.00,0.00,0.00,0.00,1.00,0.00,1.00,19,9,0.16,0.16,0.05,0.44,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def anomaly(data, normalizeFunction):\n",
    "    normalizedData = data.map(normalizeFunction)\n",
    "    normalizedData.cache()\n",
    "\n",
    "    kmeans = KMeans()\n",
    "    model = kmeans.train(normalizedData, k=150)\n",
    "\n",
    "    normalizedData.unpersist()\n",
    "\n",
    "    distances = normalizedData.map(lambda vector: distToCentroid(vector, model))\n",
    "    threshold = distances.top(100)[-1]\n",
    "\n",
    "    return lambda vector: distToCentroid(normalizeFunction(vector), model) > threshold\n",
    "\n",
    "parseFunction = onehot(rawData)\n",
    "originalAndData = rawData.map(lambda line: (line, parseFunction(line)[1]))\n",
    "data = originalAndData.values()\n",
    "normalizeFunction = normalize(data)\n",
    "anomalyFunction = anomaly(data, normalizeFunction)\n",
    "anomalies = originalAndData.filter(lambda x: anomalyFunction(x[1])).keys()\n",
    "anomalies.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4aff7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1575:=====================================>                (16 + 7) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of anomalies: 0.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fractionofanomalies = anomalies.count() / rawData.count() * 100\n",
    "print(\"Fraction of anomalies: {:.2f}%\".format(fractionofanomalies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
