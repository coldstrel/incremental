{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8e6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.sql import SparkSession\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf9c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "print(\"PySpark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae6e3a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Wiki_Parser\").getOrCreate()\n",
    "sc = spark.sparkContext    # to read input files in RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c11860ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse (analyze) the header of Wikipedia article\n",
    "\n",
    "def parseHeader(line):\n",
    "    try:\n",
    "        s = line[line.index(\"id=\\\"\") + 4:]\n",
    "        article_id = s[:s.index(\"\\\"\")]\n",
    "        s = s[s.index(\"url=\\\"\") + 5:]\n",
    "        url = s[:s.index(\"\\\"\")]\n",
    "        s = s[s.index(\"title=\\\"\") + 7:]\n",
    "        title = s[:s.index(\"\\\"\")]\n",
    "        return article_id, url, title\n",
    "    except Exception as e:\n",
    "        return \"\", \"\", \"\"\n",
    "\n",
    "# parse Wikipedia article\n",
    "def parse(lines):\n",
    "    docs = []\n",
    "    title = \"\"\n",
    "    content = \"\"\n",
    "    for line in lines:\n",
    "        try:\n",
    "            if line.startswith(\"<doc \"):\n",
    "                title = parseHeader(line)[2]\n",
    "                content = \"\"\n",
    "            elif line.startswith(\"</doc>\"):\n",
    "                if title and content:\n",
    "                    docs.append((title, content))\n",
    "            else:\n",
    "                content += line + \"\\n\"\n",
    "        except Exception as e:\n",
    "            content = \"\"\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e18d99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 0.01  # 1 percent of available files. change to 1.0 for full experiment\n",
    "numTerms = 5000    # change to 50000 for full experiment\n",
    "k = 250            # number of latent concepts in the reduced matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db76d125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(2 + 0) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "textFiles = sc.wholeTextFiles(\"../Data/enwiki-articles/*/*\").sample(False, sampleSize)\n",
    "numFiles = textFiles.count()\n",
    "\n",
    "print(\"Number of files:\", numFiles) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f371603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to parse and flatMap the text.\n",
    "\n",
    "def parse_flatMap(uri_text):\n",
    "    uri, text = uri_text\n",
    "    return parse(text.split(\"\\n\"))\n",
    "\n",
    "plainText = textFiles.flatMap(parse_flatMap)  # Assuming parse is a function defined elsewhere\n",
    "\n",
    "plainText.cache()\n",
    "numDocs = plainText.count()\n",
    "bNumDocs = sc.broadcast(numDocs)\n",
    "\n",
    "# to check if a string contains only letters?\n",
    "\n",
    "def isOnlyLetters(s):\n",
    "    return s.isalpha()\n",
    "\n",
    "# read stopwords and broadcast it.\n",
    "\n",
    "stw = set(sc.textFile(\"../Data/stopwords.txt\").collect())\n",
    "bStopWords = sc.broadcast(stw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e373e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/amir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/amir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary resources for NLTK\n",
    "download('punkt')\n",
    "download('averaged_perceptron_tagger')\n",
    "download('wordnet')\n",
    "\n",
    "stop_words_set = set(bStopWords.value)\n",
    "\n",
    "def createNLPPipeline():\n",
    "    return WordNetLemmatizer()\n",
    "\n",
    "def plainTextToLemmas(text, pipeline):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    sentences = sent_tokenize(text)            # tokenization\n",
    "\n",
    "    lemmas = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)              # word tokenization\n",
    "        for token in tokens:\n",
    "            lemma = lemmatizer.lemmatize(token.lower())    # lemmatization\n",
    "            \n",
    "            #  including lemma?\n",
    "            if len(lemma) > 2 and lemma not in stop_words_set and lemma.isalpha():\n",
    "                lemmas.append(lemma)\n",
    "\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e885c5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to perform lemmatization\n",
    "def lemmatize_text(title_contents):\n",
    "    pipeline = createNLPPipeline()\n",
    "    title, contents = title_contents\n",
    "    return (title, plainTextToLemmas(contents, pipeline))\n",
    "\n",
    "\n",
    "# register lemmatization on each partition\n",
    "lemmatized = plainText.mapPartitions(lambda it: map(lemmatize_text, it))\n",
    "\n",
    "# calculate term frequencies\n",
    "def calculate_term_freqs(title_terms):\n",
    "    title, terms = title_terms\n",
    "    termFreqs = {}\n",
    "    for term in terms:\n",
    "        termFreqs[term] = termFreqs.get(term, 0) + 1\n",
    "    return (title, termFreqs)\n",
    "\n",
    "docTermFreqs = lemmatized.map(calculate_term_freqs)\n",
    "\n",
    "# RDD\n",
    "docTermFreqs.cache()\n",
    "num_docs = docTermFreqs.count()\n",
    "print(num_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71cf17b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# bring out document IDs and zip with unique IDs\n",
    "docIds = docTermFreqs.map(lambda x: x[0]).zipWithUniqueId().map(lambda x: (x[1], x[0])).collectAsMap()\n",
    "\n",
    "#  document frequencies\n",
    "docFreqs = docTermFreqs.flatMap(lambda x: x[1].keys()).map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y, numPartitions = 24)\n",
    "\n",
    "ordering = lambda x: x[1]\n",
    "topDocFreqs = docFreqs.top(numTerms, key = ordering) # sort by frequency\n",
    "\n",
    "# IDFs\n",
    "idfs = {term: math.log(num_docs / count) for term, count in topDocFreqs}\n",
    "idTerms = dict(zip(idfs.keys(), range(len(idfs)))) # term IDs\n",
    "\n",
    "# reverse mapping of term IDs\n",
    "termIds = {v: k for k, v in idTerms.items()}\n",
    "bIdfs = sc.broadcast(idfs).value        # broadcast IDF \n",
    "bIdTerms = sc.broadcast(idTerms).value  # broadcast term mappings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c853710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "from pyspark.mllib.linalg.distributed import SingularValueDecomposition\n",
    "from typing import List, Tuple\n",
    "from pyspark.mllib.linalg import Vectors, Matrices\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90026011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/14 18:26:44 WARN RowMatrix: The input data is not directly cached, which may hurt performance if its parent RDDs are also uncached.\n",
      "24/02/14 18:31:15 WARN RowMatrix: The input data was not directly cached, which may hurt performance if its parent RDDs are also uncached.\n"
     ]
    }
   ],
   "source": [
    "# this part maps term frequencies to Vectors.\n",
    "\n",
    "vecs = docTermFreqs.map(lambda x: x[1]).map(lambda termFreqs: \n",
    "                                              Vectors.sparse(len(bIdTerms), \n",
    "                                                             [(bIdTerms[term], bIdfs[term] * termFreqs[term] / sum(termFreqs.values())) \n",
    "                                                              for term in termFreqs.keys() \n",
    "                                                              if term in bIdTerms]))\n",
    "\n",
    "vecs.cache()\n",
    "vecs.count()\n",
    "\n",
    "mat = RowMatrix(vecs)                     # RowMatrix from vecs\n",
    "svd = mat.computeSVD(k, computeU = True)  # to compute SVD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6499fa",
   "metadata": {},
   "source": [
    "## Query the Latent Semantic Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15aad6c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept terms: , , , , , , , , , , , \n",
      "Concept docs: Red blood cell, , Beastie Boys, Bluetooth, Skyscraper, Bourbon, , Jellyfish, Desegregation, Holy Roman Emperor, Spinel, Biotin\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Beastie Boys, British Isles, Alkanna tinctoria, MÄori language, Polymer, Transphobia, Pope Liberius, Anzac biscuit, Bhangra (music), Berlin Wall, , Carl Lewis\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Bluetooth, Red blood cell, , Skyscraper, Bourbon, , Jellyfish, Carl Lewis, Sonja Henie, Martian meteorite, Andaman Sea, Salvation\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Red blood cell, , Skyscraper, Bourbon, , Jellyfish, Desegregation, Holy Roman Emperor, Spinel, Biotin, Apollonius of Tyana, Sonja Henie\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Holy Roman Emperor, Pulitzer Prize for History, Income tax, Philosophy of religion, Preliminary hearing, Black Holes and Baby Universes and Other Essays, Panama, Louis de Broglie, William McGonagall, Ritual Entertainment, Battle of Stoke Field, Wyoming\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Red blood cell, , Sonja Henie, Skyscraper, Bourbon, , Jellyfish, Biotin, Apollonius of Tyana, Salvation, Basque language, Kate Chopin\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Sonja Henie, Desegregation, Red blood cell, , Skyscraper, Bourbon, , Jellyfish, Biotin, Apollonius of Tyana, Salvation, Kaman SH-2 Seasprite\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Red blood cell, , Skyscraper, Bourbon, , Jellyfish, Biotin, Apollonius of Tyana, Salvation, Pulp Fiction, Kate Chopin, Wakeboarding\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Biotin, Kate Chopin, SPARC, Wakeboarding, Speech recognition, Grand Canal, Tianjin, Desegregation, Holy Roman Emperor, Cappuccino, Spinel, Sonja Henie\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Biotin, Holy Roman Emperor, Desegregation, Spinel, Red blood cell, , Folic acid, Sonja Henie, Skyscraper, Bourbon, , Jellyfish\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: Salvation, General Dynamics Electric Boat, , William S. Burroughs, Tianjin, Arabesque, George Farquhar, Columbia, Electric boat, Politics of Tunisia, A-wing, Tony Hancock\n",
      "\n",
      "Concept terms: \n",
      "Concept docs: A-wing, Columbia, Manta ray, Benny Andersson, Zebu, , Sapphire, Martian meteorite, Warsaw Pact, Yuri Andropov, Bass (sound), Placenta\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def topTermsInTopConcepts(svd: SingularValueDecomposition, numConcepts: int, numTerms: int) -> List[List[Tuple[str, float]]]:\n",
    "    v = svd.V\n",
    "    topTerms = []\n",
    "    arr = v.toArray()\n",
    "    \n",
    "    for i in range(numConcepts):\n",
    "        \n",
    "        offs = i * v.numRows\n",
    "                \n",
    "        termWeights = [(arr[offs + j], j) for j in range(v.numRows) if offs + j < len(arr)]\n",
    "        \n",
    "        # Correct the sorting to consider the actual score of each term\n",
    "        sorted_terms = sorted(termWeights, key = lambda x: -x[0][0])\n",
    "        \n",
    "        topTerms.append([(bIdTerms.get(idx, (\"\", -1))[0], score[0]) for score, idx in sorted_terms[:numTerms]])\n",
    "    \n",
    "    return topTerms\n",
    "\n",
    "def topDocsInTopConcepts(svd: SingularValueDecomposition, numConcepts: int, numDocs: int) -> List[List[Tuple[str, float]]]:\n",
    "    u = svd.U\n",
    "    topDocs = []\n",
    "    \n",
    "    for i in range(numConcepts):\n",
    "        docWeights = [(score, doc_id) for score, doc_id in zip(u.rows.map(lambda row: row.toArray()[i]).collect(), range(u.numRows()))]\n",
    "        sorted_docs = sorted(docWeights, key = lambda x: -x[0])\n",
    "        topDocs.append([(docIds.get(idx, \"\"), score) for score, idx in sorted_docs[:numDocs]])\n",
    "    \n",
    "    return topDocs\n",
    "\n",
    "\n",
    "top_concept_terms = topTermsInTopConcepts(svd, 12, 12)\n",
    "top_concept_docs = topDocsInTopConcepts(svd, 12, 12)\n",
    "\n",
    "for terms, docs in zip(top_concept_terms, top_concept_docs):\n",
    "    print(\"Concept terms: \" + \", \".join([term for term, _ in terms]))\n",
    "    print(\"Concept docs: \" + \", \".join([doc for doc, _ in docs]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4fb0c",
   "metadata": {},
   "source": [
    "##  Keyword Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30061c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.09522831958479537, 94),\n",
       " (0.017633439482589663, 207),\n",
       " (0.013101517516039005, 64),\n",
       " (0.01198477444436551, 209),\n",
       " (0.01160014620839545, 55),\n",
       " (0.010327259323650608, 271),\n",
       " (0.007201631651629089, 305),\n",
       " (0.006516759976356174, 4),\n",
       " (0.0058830638907972806, 37),\n",
       " (0.005283032294726741, 24)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def termsToQueryVector(terms, idTerms, idfs):\n",
    "    indices = [idTerms[term] for term in terms]\n",
    "    values = [idfs[term] for term in terms]\n",
    "    return csr_matrix((values, (indices, [0]*len(indices))), shape=(len(idTerms), 1))\n",
    "\n",
    "def topDocsForTermQuery(US, V, query):\n",
    "    term_row_arr = np.dot(V.toArray().T, query.toarray()).flatten()\n",
    "    term_row_vec = Matrices.dense(len(term_row_arr), 1, term_row_arr)\n",
    "    doc_scores = US.multiply(term_row_vec)\n",
    "    all_doc_weights = doc_scores.rows.zipWithUniqueId().map(lambda x: (x[0].toArray()[0], x[1]))\n",
    "    return sorted(all_doc_weights.collect(), key=lambda x: -x[0])[:10]\n",
    "\n",
    "def multiplyByDiagonalRowMatrix(mat, diag):\n",
    "    s_arr = diag.toArray()\n",
    "    return RowMatrix(mat.rows.map(lambda vec: Vectors.dense(np.multiply(vec.toArray(), s_arr))))\n",
    "\n",
    "US = multiplyByDiagonalRowMatrix(svd.U, svd.s)\n",
    "\n",
    "terms = [\"serious\", \"incident\"]\n",
    "queryVec = termsToQueryVector(terms, idTerms, idfs)\n",
    "topDocsForTermQuery(US, svd.V, queryVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa7f76ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of matrix US:\n",
      "Number of rows: 283\n",
      "Number of columns: 250\n",
      "\n",
      "Dimensions of matrix V:\n",
      "Number of rows: 5000\n",
      "Number of columns: 250\n"
     ]
    }
   ],
   "source": [
    "# dimensions of matrix US\n",
    "num_rows_US = US.numRows()\n",
    "num_cols_US = US.numCols()\n",
    "\n",
    "# dimensions of matrix V\n",
    "num_rows_V = svd.V.numRows\n",
    "num_cols_V = svd.V.numCols\n",
    "\n",
    "print(\"Dimensions of matrix US:\")\n",
    "print(f\"Number of rows: {num_rows_US}\")\n",
    "print(f\"Number of columns: {num_cols_US}\")\n",
    "\n",
    "print(\"\\nDimensions of matrix V:\")\n",
    "print(f\"Number of rows: {num_rows_V}\")\n",
    "print(f\"Number of columns: {num_cols_V}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
